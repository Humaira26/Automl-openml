{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, OrderedDict, deque\n",
    "import copy\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.linalg import LinAlgError\n",
    "import scipy.sparse\n",
    "import sklearn\n",
    "# TODO use balanced accuracy!\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import openml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "from autosklearn.pipeline.implementations.OneHotEncoder import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Allow multiple dependencies for a metafeature\n",
    "# TODO Add HelperFunction as an object\n",
    "class HelperFunctions(object):\n",
    "    def __init__(self):\n",
    "        self.functions = OrderedDict()\n",
    "        self.values = OrderedDict()\n",
    "\n",
    "    def clear(self):\n",
    "        self.values = OrderedDict()\n",
    "        self.computation_time = OrderedDict()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.functions.__iter__()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.functions.__getitem__(item)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        return self.functions.__setitem__(key, value)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        return self.functions.__delitem__(key)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return self.functions.__contains__(item)\n",
    "\n",
    "    def is_calculated(self, key):\n",
    "        \"\"\"Return if a helper function has already been executed.\n",
    "        Necessary as get_value() can return None if the helper function hasn't\n",
    "        been executed or if it returned None.\"\"\"\n",
    "        return key in self.values\n",
    "\n",
    "    def get_value(self, key):\n",
    "        return self.values.get(key).value\n",
    "\n",
    "    def set_value(self, key, item):\n",
    "        self.values[key] = item\n",
    "\n",
    "    def define(self, name):\n",
    "        \"\"\"Decorator for adding helper functions to a \"dictionary\".\n",
    "        This behaves like a function decorating a function,\n",
    "        not a class decorating a function\"\"\"\n",
    "        def wrapper(metafeature_class):\n",
    "            instance = metafeature_class()\n",
    "            self.__setitem__(name, instance)\n",
    "            return instance\n",
    "        return wrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetafeatureFunctions(object):\n",
    "    def __init__(self):\n",
    "        self.functions = OrderedDict()\n",
    "        self.dependencies = OrderedDict()\n",
    "        self.values = OrderedDict()\n",
    "\n",
    "    def clear(self):\n",
    "        self.values = OrderedDict()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.functions.__iter__()\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.functions.__getitem__(item)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        return self.functions.__setitem__(key, value)\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        return self.functions.__delitem__(key)\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        return self.functions.__contains__(item)\n",
    "\n",
    "    def get_value(self, key):\n",
    "        return self.values[key].value\n",
    "\n",
    "    def set_value(self, key, item):\n",
    "        self.values[key] = item\n",
    "\n",
    "    def is_calculated(self, key):\n",
    "        \"\"\"Return if a helper function has already been executed.\n",
    "        Necessary as get_value() can return None if the helper function hasn't\n",
    "        been executed or if it returned None.\"\"\"\n",
    "        return key in self.values\n",
    "\n",
    "    def get_dependency(self, name):\n",
    "        \"\"\"Return the dependency of metafeature \"name\".\n",
    "        \"\"\"\n",
    "        return self.dependencies.get(name)\n",
    "\n",
    "    def define(self, name, dependency=None):\n",
    "        \"\"\"Decorator for adding metafeature functions to a \"dictionary\" of\n",
    "        metafeatures. This behaves like a function decorating a function,\n",
    "        not a class decorating a function\"\"\"\n",
    "        def wrapper(metafeature_class):\n",
    "            instance = metafeature_class()\n",
    "            self.__setitem__(name, instance)\n",
    "            self.dependencies[name] = dependency\n",
    "            return instance\n",
    "        return wrapper\n",
    "    \n",
    "metafeatures = MetafeatureFunctions()\n",
    "helper_functions = HelperFunctions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task(task_id):\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    X, y = task.get_X_and_y()\n",
    "    train_indices, test_indices = task.get_train_test_split_indices()\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    dataset = openml.datasets.get_dataset(task.dataset_id)\n",
    "    _, _, cat = dataset.get_data(return_categorical_indicator=True,\n",
    "target=task.target_name)\n",
    "    del _\n",
    "    del dataset\n",
    "    cat = ['categorical' if c else 'numerical' for c in cat]\n",
    "\n",
    "    unique = np.unique(y_train)\n",
    "    mapping = {unique_value: i for i, unique_value in enumerate(unique)}\n",
    "    y_train = np.array([mapping[value] for value in y_train])\n",
    "    y_test = np.array([mapping[value] for value in y_test])\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test, cat = load_task(75182)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumberOfInstances\n",
      "LogNumberOfInstances\n",
      "NumberofClasses\n",
      "NumberOfFeatures\n",
      "LogNumberOfFeatures\n",
      "MissingValues\n",
      "NumberOfInstancesWithMissingValues\n",
      "PercentageOfInstancesWithMissingValues\n",
      "NumberOfFeaturesWithMissingValues\n",
      "PercentageOfFeaturesWithMissingValues\n",
      "NumberOfMissingValues\n",
      "PercentageOfMissingValues\n",
      "NumberOfNumericFeatures\n",
      "NumberOfCategoricalFeatures\n",
      "RatioNumericalToNominal\n",
      "DatasetRatio\n",
      "LogDatasetRatio\n",
      "InverseDatasetRatio\n",
      "LogInverseDatasetRatio\n",
      "CPU times: user 32.1 ms, sys: 99 Âµs, total: 32.2 ms\n",
      "Wall time: 28.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Simple features\n",
    "computed_features={}\n",
    "def NumberOfInstances(X, y, categorical):\n",
    "    return float(X.shape[0])\n",
    "computed_features['NumberOfInstances']=NumberOfInstances(X_train,y_train,cat)\n",
    "print('NumberOfInstances')\n",
    "\n",
    "def LogNumberOfInstances(X, y, categorical):\n",
    "    return np.log(computed_features[\"NumberOfInstances\"])\n",
    "computed_features['LogNumberOfInstances']=LogNumberOfInstances(X_train,y_train,cat)\n",
    "print('LogNumberOfInstances')\n",
    "    \n",
    "#Calculate the number of classes.\n",
    "#Calls np.unique on the targets. If the dataset is a multilabel dataset,\n",
    "#does this for each label seperately and returns the mean.    \n",
    "def NumberofClasses(X, y, categorical):\n",
    "    if len(y.shape) == 2:\n",
    "        return np.mean([len(np.unique(y[:,i])) for i in range(y.shape[1])])\n",
    "    else:\n",
    "        return float(len(np.unique(y)))\n",
    "computed_features['NumberofClasses']=NumberofClasses(X_train,y_train,cat)\n",
    "print('NumberofClasses')\n",
    "\n",
    "\n",
    "def NumberOfFeatures(X, y, categorical):\n",
    "    return float(X.shape[1])\n",
    "computed_features['NumberOfFeatures'] = NumberOfFeatures(X_train,y_train,cat)\n",
    "print('NumberOfFeatures')\n",
    "\n",
    "\n",
    "def LogNumberOfFeatures(X,y,categorical):\n",
    "    return np.log(computed_features['NumberOfFeatures'] )\n",
    "computed_features['LogNumberOfFeatures'] = LogNumberOfFeatures(X_train,y_train,cat)\n",
    "print('LogNumberOfFeatures')\n",
    "\n",
    "def MissingValues(X, y, categorical):\n",
    "    missing = ~np.isfinite(X)\n",
    "    return missing\n",
    "computed_features['MissingValues'] = MissingValues(X_train,y_train,cat)\n",
    "print('MissingValues')\n",
    "\n",
    "def NumberOfInstancesWithMissingValues(X, y, categorical):\n",
    "    missing = computed_features[\"MissingValues\"]\n",
    "    num_missing = missing.sum(axis=1)\n",
    "    return float(np.sum([1 if num > 0 else 0 for num in num_missing]))\n",
    "computed_features['NumberOfInstancesWithMissingValues'] = NumberOfInstancesWithMissingValues(X_train,y_train,cat)\n",
    "print('NumberOfInstancesWithMissingValues')\n",
    "\n",
    "\n",
    "def PercentageOfInstancesWithMissingValues(X, y, categorical):\n",
    "    return float(computed_features[\"NumberOfInstancesWithMissingValues\"]) / float(computed_features[\"NumberOfInstances\"])\n",
    "computed_features['PercentageOfInstancesWithMissingValues'] = PercentageOfInstancesWithMissingValues(X_train,y_train,cat)\n",
    "print('PercentageOfInstancesWithMissingValues')\n",
    "\n",
    "\n",
    "\n",
    "def NumberOfFeaturesWithMissingValues(X, y, categorical):\n",
    "        missing = computed_features[\"MissingValues\"]\n",
    "        num_missing = missing.sum(axis=0)\n",
    "        return float(np.sum([1 if num > 0 else 0 for num in num_missing]))\n",
    "computed_features['NumberOfFeaturesWithMissingValues'] = NumberOfFeaturesWithMissingValues(X_train,y_train,cat)\n",
    "print('NumberOfFeaturesWithMissingValues')\n",
    "\n",
    "def PercentageOfFeaturesWithMissingValues(X, y, categorical):\n",
    "        return float(computed_features[\"NumberOfFeaturesWithMissingValues\"]) / float(computed_features[\"NumberOfFeatures\"])\n",
    "computed_features['PercentageOfFeaturesWithMissingValues'] = PercentageOfFeaturesWithMissingValues(X_train,y_train,cat)\n",
    "print('PercentageOfFeaturesWithMissingValues')\n",
    "\n",
    "\n",
    "def NumberOfMissingValues(X, y, categorical):\n",
    "        return float(computed_features[\"MissingValues\"].sum())\n",
    "computed_features['NumberOfMissingValues'] = NumberOfMissingValues(X_train,y_train,cat)\n",
    "print('NumberOfMissingValues')\n",
    "\n",
    "\n",
    "def PercentageOfMissingValues(X, y, categorical):\n",
    "        return float(computed_features[\"NumberOfMissingValues\"]) / float(X.shape[0]*X.shape[1])\n",
    "computed_features['PercentageOfMissingValues'] = PercentageOfMissingValues(X_train,y_train,cat)\n",
    "print('PercentageOfMissingValues')\n",
    "\n",
    "\n",
    "def NumberOfNumericFeatures(X, y, categorical):\n",
    "    numerical_features=0\n",
    "    for i in categorical:\n",
    "        if i=='numerical':\n",
    "            numerical_features+=1\n",
    "    return numerical_features\n",
    "computed_features['NumberOfNumericFeatures'] = NumberOfNumericFeatures(X_train,y_train,cat)\n",
    "print('NumberOfNumericFeatures')\n",
    "\n",
    "def NumberOfCategoricalFeatures(X, y, categorical):\n",
    "    categorical_features=0\n",
    "    for i in categorical:\n",
    "        if i=='categorical':\n",
    "            categorical_features+=1\n",
    "    return categorical_features\n",
    "computed_features['NumberOfCategoricalFeatures'] = NumberOfCategoricalFeatures(X_train,y_train,cat)\n",
    "print('NumberOfCategoricalFeatures')\n",
    "\n",
    "def RatioNumericalToNominal(X, y, categorical):\n",
    "        num_categorical = float(computed_features[\"NumberOfCategoricalFeatures\"])\n",
    "        num_numerical = float(computed_features[\"NumberOfNumericFeatures\"])\n",
    "        if num_categorical == 0.0:\n",
    "            return 0.\n",
    "        return num_numerical / num_categorical\n",
    "computed_features['RatioNumericalToNominal'] = RatioNumericalToNominal(X_train,y_train,cat)\n",
    "print('RatioNumericalToNominal')\n",
    "\n",
    "\n",
    "# Number of attributes divided by number of samples\n",
    "\n",
    "def DatasetRatio(X, y, categorical):\n",
    "        return float(computed_features[\"NumberOfFeatures\"]) / float(computed_features[\"NumberOfInstances\"])\n",
    "computed_features['DatasetRatio'] = DatasetRatio(X_train,y_train,cat)\n",
    "print('DatasetRatio')\n",
    "                     \n",
    "def LogDatasetRatio(X, y, categorical):\n",
    "        return np.log(computed_features[\"DatasetRatio\"])\n",
    "computed_features['LogDatasetRatio'] = LogDatasetRatio(X_train,y_train,cat)\n",
    "print('LogDatasetRatio')                 \n",
    "                     \n",
    "def InverseDatasetRatio(X, y, categorical):\n",
    "        return float(computed_features[\"NumberOfInstances\"]) / float(computed_features[\"NumberOfFeatures\"])\n",
    "computed_features['InverseDatasetRatio'] = InverseDatasetRatio(X_train,y_train,cat)\n",
    "print('InverseDatasetRatio')\n",
    "                     \n",
    "def LogInverseDatasetRatio(X, y, categorical):\n",
    "        return np.log(computed_features[\"InverseDatasetRatio\"])\n",
    "computed_features['LogInverseDatasetRatio'] = LogInverseDatasetRatio(X_train,y_train,cat)\n",
    "print('LogInverseDatasetRatio')\n",
    "                     \n",
    "def ClassOccurences(X, y, categorical):\n",
    "        if len(y.shape) == 2:\n",
    "            occurences = []\n",
    "            for i in range(y.shape[1]):\n",
    "                occurences.append(self._calculate(X, y[:, i], cat))\n",
    "            return occurences\n",
    "        else:\n",
    "            occurence_dict = defaultdict(float)\n",
    "            for value in y:\n",
    "                occurence_dict[value] += 1\n",
    "            return occurence_dict\n",
    "computed_features['ClassOccurences'] = ClassOccurences(X_train,y_train,cat)\n",
    "\n",
    "def ClassProbabilityMin(X, y, categorical):\n",
    "        occurences = computed_features[\"ClassOccurences\"]\n",
    "\n",
    "        min_value = np.iinfo(np.int64).max\n",
    "        if len(y.shape) == 2:\n",
    "            for i in range(y.shape[1]):\n",
    "                for num_occurences in occurences[i].values():\n",
    "                    if num_occurences < min_value:\n",
    "                        min_value = num_occurences\n",
    "        else:\n",
    "            for num_occurences in occurences.values():\n",
    "                if num_occurences < min_value:\n",
    "                    min_value = num_occurences\n",
    "        return float(min_value) / float(y.shape[0])\n",
    "                     \n",
    "computed_features['ClassProbabilityMin'] = ClassProbabilityMin(X_train,y_train,cat)\n",
    "\n",
    "# aka default accuracy\n",
    "\n",
    "def ClassProbabilityMax(X, y, categorical):\n",
    "        occurences = computed_features[\"ClassOccurences\"]\n",
    "        max_value = -1\n",
    "\n",
    "        if len(y.shape) == 2:\n",
    "            for i in range(y.shape[1]):\n",
    "                for num_occurences in occurences[i].values():\n",
    "                    if num_occurences > max_value:\n",
    "                        max_value = num_occurences\n",
    "        else:\n",
    "            for num_occurences in occurences.values():\n",
    "                if num_occurences > max_value:\n",
    "                    max_value = num_occurences\n",
    "        return float(max_value) / float(y.shape[0])\n",
    "                     \n",
    "computed_features['ClassProbabilityMax'] = ClassProbabilityMax(X_train,y_train,cat)\n",
    "\n",
    "def ClassProbabilityMean(X, y, categorical):\n",
    "        occurence_dict = computed_features[\"ClassOccurences\"]\n",
    "        if len(y.shape) == 2:\n",
    "            occurences = []\n",
    "            for i in range(y.shape[1]):\n",
    "                occurences.extend(\n",
    "                    [occurrence for occurrence in occurence_dict[\n",
    "                        i].values()])\n",
    "            occurences = np.array(occurences)\n",
    "        else:\n",
    "            occurences = np.array([occurrence for occurrence in occurence_dict.values()],\n",
    "                                  dtype=np.float64)\n",
    "        return (occurences / y.shape[0]).mean()\n",
    "computed_features['ClassProbabilityMean'] = ClassProbabilityMean(X_train,y_train,cat)\n",
    "\n",
    "def ClassProbabilitySTD(X, y, categorical):\n",
    "        occurence_dict = computed_features[\"ClassOccurences\"]\n",
    "\n",
    "        if len(y.shape) == 2:\n",
    "            stds = []\n",
    "            for i in range(y.shape[1]):\n",
    "                std = np.array(\n",
    "                    [occurrence for occurrence in occurence_dict[\n",
    "                                                      i].values()],\n",
    "                    dtype=np.float64)\n",
    "                std = (std / y.shape[0]).std()\n",
    "                stds.append(std)\n",
    "            return np.mean(stds)\n",
    "        else:\n",
    "            occurences = np.array([occurrence for occurrence in occurence_dict.values()],\n",
    "                                 dtype=np.float64)\n",
    "            return (occurences / y.shape[0]).std()\n",
    "computed_features['ClassProbabilitySTD'] = ClassProbabilitySTD(X_train,y_train,cat)\n",
    "\n",
    "\n",
    "\n",
    "def NumSymbols(X, y, categorical):\n",
    "        symbols_per_column = []\n",
    "        for i, column in enumerate(X.T):\n",
    "            if categorical[i]:\n",
    "                unique_values = np.unique(column)\n",
    "                num_unique = np.sum(np.isfinite(unique_values))\n",
    "                symbols_per_column.append(num_unique)\n",
    "        return symbols_per_column\n",
    "    \n",
    "computed_features['NumSymbols'] = NumSymbols(X_train,y_train,cat)  \n",
    "\n",
    "\n",
    "def SymbolsMin(X, y, categorical):\n",
    "        # The minimum can only be zero if there are no nominal features,\n",
    "        # otherwise it is at least one\n",
    "        # TODO: shouldn't this rather be two?\n",
    "        minimum = None\n",
    "        for unique in computed_features['NumSymbols']:\n",
    "            if unique > 0 and (minimum is None or unique < minimum):\n",
    "                minimum = unique\n",
    "        return minimum if minimum is not None else 0\n",
    "    \n",
    "computed_features['SymbolsMin'] = SymbolsMin(X_train,y_train,cat)  \n",
    "\n",
    "def SymbolsMax(X, y, categorical):\n",
    "        values = computed_features['NumSymbols']\n",
    "        if len(values) == 0:\n",
    "            return 0\n",
    "        return max(max(values), 0)\n",
    "    \n",
    "computed_features['SymbolsMax'] = SymbolsMax(X_train,y_train,cat)     \n",
    "\n",
    "def SymbolsMean(X, y, categorical):\n",
    "        # TODO: categorical attributes without a symbol don't count towards this\n",
    "        # measure\n",
    "        values = [val for val in computed_features['NumSymbols'] if val > 0]\n",
    "        mean = np.nanmean(values)\n",
    "        return mean if np.isfinite(mean) else 0\n",
    "    \n",
    "computed_features['SymbolsMean'] = SymbolsMean(X_train,y_train,cat)\n",
    "\n",
    "def SymbolsSTD(X, y, categorical):\n",
    "        values = [val for val in computed_features['NumSymbols'] if val > 0]\n",
    "        std = np.nanstd(values)\n",
    "        return std if np.isfinite(std) else 0\n",
    "    \n",
    "computed_features['SymbolsSTD'] = SymbolsSTD(X_train,y_train,cat)   \n",
    "\n",
    "\n",
    "def SymbolsSum(X, y, categorical):\n",
    "        sum = np.nansum(computed_features['NumSymbols'])\n",
    "        return sum if np.isfinite(sum) else 0\n",
    "    \n",
    "computed_features['SymbolsSum'] = SymbolsSum(X_train,y_train,cat)   \n",
    "\n",
    "\n",
    "def ClassEntropy(X, y, categorical):\n",
    "        labels = 1 if len(y.shape) == 1 else y.shape[1]\n",
    "        if labels == 1:\n",
    "            y = y.reshape((-1, 1))\n",
    "\n",
    "        entropies = []\n",
    "        for i in range(labels):\n",
    "            occurence_dict = defaultdict(float)\n",
    "            for value in y[:, i]:\n",
    "                occurence_dict[value] += 1\n",
    "            entropies.append(scipy.stats.entropy([occurence_dict[key] for key in\n",
    "                                                 occurence_dict], base=2))\n",
    "\n",
    "        return np.mean(entropies)\n",
    "\n",
    "computed_features['ClassEntropy'] = ClassEntropy(X_train,y_train,cat)   \n",
    "\n",
    "def mutual_information(X, y, categorical):\n",
    "    try:\n",
    "        if categorical == -1:\n",
    "            categorical = doane_cat(x)\n",
    "        if categorical == np.inf:\n",
    "            categorical = sturges_cat(x)\n",
    "    except ValueError:\n",
    "        categorical = 10.0\n",
    "    # print \"bins\", bins\n",
    "    try:\n",
    "        c_Xy = np.histogram2d(x, y, categorical)[0]\n",
    "        mi = metrics.mutual_info_score(None, None, contingency=c_xy)\n",
    "        # print \"success\"\n",
    "    except Exception: \n",
    "        #print \"error with mi calc\", str(e)\n",
    "        mi = 0\n",
    "    return mi\n",
    "\n",
    "computed_features['mutual_information'] = mutual_information(X_train,y_train,cat)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Kurtosisses(X, y, categorical):\n",
    "        kurts = []\n",
    "        for i in range(X.shape[1]):\n",
    "            if not categorical[i]:\n",
    "                kurts.append(scipy.stats.kurtosis(X[:, i]))\n",
    "        return kurts\n",
    "computed_features['Kurtosisses'] = Kurtosisses(X_train,y_train,cat)\n",
    "    \n",
    "def KurtosisMin(X, y, categorical):\n",
    "        kurts = computed_features[\"Kurtosisses\"]\n",
    "        minimum = np.nanmin(kurts) if len(kurts) > 0 else 0\n",
    "        return minimum if np.isfinite(minimum) else 0\n",
    "computed_features['KurtosisMin'] = KurtosisMin(X_train,y_train,cat)\n",
    "\n",
    "\n",
    "def KurtosisMax(X, y, categorical):\n",
    "        kurts = computed_features[\"Kurtosisses\"]\n",
    "        maximum = np.nanmax(kurts) if len(kurts) > 0 else 0\n",
    "        return maximum if np.isfinite(maximum) else 0\n",
    "computed_features['KurtosisMax'] = KurtosisMax(X_train,y_train,cat)\n",
    "\n",
    "\n",
    "def KurtosisMean(X, y, categorical):\n",
    "        kurts = computed_features[\"Kurtosisses\"]\n",
    "        mean = np.nanmean(kurts) if len(kurts) > 0 else 0\n",
    "        return mean if np.isfinite(mean) else 0\n",
    "computed_features['KurtosisMean'] = KurtosisMean(X_train,y_train,cat) \n",
    "    \n",
    "def KurtosisSTD(X, y, categorical):\n",
    "        kurts = computed_features[\"Kurtosisses\"]\n",
    "        std = np.nanstd(kurts) if len(kurts) > 0 else 0\n",
    "        return std if np.isfinite(std) else 0\n",
    "computed_features['KurtosisSTD'] = KurtosisSTD(X_train,y_train,cat) \n",
    "\n",
    "def Skewnesses(X, y, categorical):\n",
    "        skews = []\n",
    "        for i in range(X.shape[1]):\n",
    "            if not categorical[i]:\n",
    "                skews.append(scipy.stats.skew(X[:, i]))\n",
    "        return skews\n",
    "computed_features['Skewnesses'] = Skewnesses(X_train,y_train,cat) \n",
    "    \n",
    "\n",
    "def SkewnessMin(X, y, categorical):\n",
    "        skews = computed_features[\"Skewnesses\"]\n",
    "        minimum = np.nanmin(skews) if len(skews) > 0 else 0\n",
    "        return minimum if np.isfinite(minimum) else 0\n",
    "computed_features['SkewnessMin'] = SkewnessMin(X_train,y_train,cat) \n",
    "\n",
    "def SkewnessMax(X, y, categorical):\n",
    "        skews = computed_features[\"Skewnesses\"]\n",
    "        maximum = np.nanmax(skews) if len(skews) > 0 else 0\n",
    "        return maximum if np.isfinite(maximum) else 0\n",
    "computed_features['SkewnessMax'] = SkewnessMax(X_train,y_train,cat)    \n",
    "\n",
    "def SkewnessMean(X, y, categorical):\n",
    "        skews = computed_features[\"Skewnesses\"]\n",
    "        mean = np.nanmean(skews) if len(skews) > 0 else 0\n",
    "        return mean if np.isfinite(mean) else 0\n",
    "computed_features['SkewnessMean'] = SkewnessMean(X_train,y_train,cat)    \n",
    "\n",
    "def SkewnessSTD(X, y, categorical):\n",
    "        skews = computed_features[\"Skewnesses\"]\n",
    "        std = np.nanstd(skews) if len(skews) > 0 else 0\n",
    "        return std if np.isfinite(std) else 0\n",
    "computed_features['SkewnessSTD'] = SkewnessSTD(X_train,y_train,cat)  \n",
    "\n",
    "\n",
    "def LandmarkLDA(X, y, categorical):\n",
    "        import sklearn.discriminant_analysis\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        try:\n",
    "            for train, test in kf.split(X, y):\n",
    "                lda = sklearn.discriminant_analysis.LinearDiscriminantAnalysis()\n",
    "\n",
    "                if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                    lda.fit(X[train], y[train])\n",
    "                else:\n",
    "                    lda = OneVsRestClassifier(lda)\n",
    "                    lda.fit(X[train], y[train])\n",
    "\n",
    "                predictions = lda.predict(X[test])\n",
    "                accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "            return accuracy / 10\n",
    "        except scipy.linalg.LinAlgError as e:\n",
    "            self.logger.warning(\"LDA failed: %s Returned 0 instead!\" % e)\n",
    "            return np.NaN\n",
    "        except ValueError as e:\n",
    "            self.logger.warning(\"LDA failed: %s Returned 0 instead!\" % e)\n",
    "            return np.NaN\n",
    "computed_features['LandmarkLDA'] = LandmarkLDA(X_train,y_train,cat)  \n",
    "\n",
    "# Naive Bayes\n",
    "def LandmarkNaiveBayes(X, y, categorical):\n",
    "        import sklearn.naive_bayes\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        for train, test in kf.split(X, y):\n",
    "            nb = sklearn.naive_bayes.GaussianNB()\n",
    "\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                nb.fit(X[train], y[train])\n",
    "            else:\n",
    "                nb = OneVsRestClassifier(nb)\n",
    "                nb.fit(X[train], y[train])\n",
    "\n",
    "            predictions = nb.predict(X[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "computed_features['LandmarkNaiveBayes'] = LandmarkNaiveBayes(X_train,y_train,cat) \n",
    "   \n",
    "\n",
    "def LandmarkDecisionTree(X, y, categorical):\n",
    "        import sklearn.tree\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        for train, test in kf.split(X, y):\n",
    "            random_state = sklearn.utils.check_random_state(42)\n",
    "            tree = sklearn.tree.DecisionTreeClassifier(random_state=random_state)\n",
    "\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                tree.fit(X[train], y[train])\n",
    "            else:\n",
    "                tree = OneVsRestClassifier(tree)\n",
    "                tree.fit(X[train], y[train])\n",
    "\n",
    "            predictions = tree.predict(X[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "computed_features['LandmarkDecisionTree'] = LandmarkDecisionTree(X_train,y_train,cat)\n",
    "\n",
    "\"\"\"If there is a dataset which has OneHotEncoded features it can happend that\n",
    "a node learner splits at one of the attribute encodings. This should be fine\n",
    "as the dataset is later on used encoded.\"\"\"\n",
    "\n",
    "# TODO: use the same tree, this has then to be computed only once and hence\n",
    "#  saves a lot of time...\n",
    "\n",
    "def LandmarkDecisionNodeLearner(X, y, categorical):\n",
    "        import sklearn.tree\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        for train, test in kf.split(X, y):\n",
    "            random_state = sklearn.utils.check_random_state(42)\n",
    "            node = sklearn.tree.DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=1, random_state=random_state,\n",
    "                min_samples_split=2, min_samples_leaf=1,  max_features=None)\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                node.fit(X[train], y[train])\n",
    "            else:\n",
    "                node = OneVsRestClassifier(node)\n",
    "                node.fit(X[train], y[train])\n",
    "            predictions = node.predict(X[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "computed_features['LandmarkDecisionNodeLearner'] = LandmarkDecisionNodeLearner(X_train,y_train,cat)\n",
    "\n",
    "def LandmarkRandomNodeLearner(X, y, categorical):\n",
    "        import sklearn.tree\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "        accuracy = 0.\n",
    "\n",
    "        for train, test in kf.split(X, y):\n",
    "            random_state = sklearn.utils.check_random_state(42)\n",
    "            node = sklearn.tree.DecisionTreeClassifier(\n",
    "                criterion=\"entropy\", max_depth=1, random_state=random_state,\n",
    "                min_samples_split=2, min_samples_leaf=1, max_features=1)\n",
    "            node.fit(X[train], y[train])\n",
    "            predictions = node.predict(X[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "\n",
    "computed_features['LandmarkRandomNodeLearner'] = LandmarkRandomNodeLearner(X_train,y_train,cat)\n",
    "\n",
    "\n",
    "# Replace the Elite 1NN with a normal 1NN, this slightly changes the\n",
    "# intuition behind this landmark, but Elite 1NN is used nowhere else...\n",
    "def Landmark1NN(X, y, categorical):\n",
    "        import sklearn.neighbors\n",
    "\n",
    "        if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "            kf = sklearn.model_selection.StratifiedKFold(n_splits=10)\n",
    "        else:\n",
    "            kf = sklearn.model_selection.KFold(n_splits=10)\n",
    "\n",
    "        accuracy = 0.\n",
    "        for train, test in kf.split(X, y):\n",
    "            kNN = sklearn.neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "            if len(y.shape) == 1 or y.shape[1] == 1:\n",
    "                kNN.fit(X[train], y[train])\n",
    "            else:\n",
    "                kNN = OneVsRestClassifier(kNN)\n",
    "                kNN.fit(X[train], y[train])\n",
    "            predictions = kNN.predict(X[test])\n",
    "            accuracy += sklearn.metrics.accuracy_score(predictions, y[test])\n",
    "        return accuracy / 10\n",
    "\n",
    "computed_features['Landmark1NN'] = Landmark1NN(X_train,y_train,cat)\n",
    "\n",
    "# Bardenet 2013 - Collaborative Hyperparameter Tuning\n",
    "# K number of classes (\"number_of_classes\")\n",
    "# log(d), log(number of attributes)\n",
    "# log(n/d), log(number of training instances/number of attributes)\n",
    "# p, how many principal components to keep in order to retain 95% of the\n",
    "#     dataset variance\n",
    "# skewness of a dataset projected onto one principal component...\n",
    "# kurtosis of a dataset projected onto one principal component\n",
    "\n",
    "def PCA(X, y, categorical):\n",
    "        import sklearn.decomposition\n",
    "        pca = sklearn.decomposition.PCA(copy=True)\n",
    "        rs = np.random.RandomState(42)\n",
    "        indices = np.arange(X.shape[0])\n",
    "        for i in range(10):\n",
    "            try:\n",
    "                rs.shuffle(indices)\n",
    "                pca.fit(X[indices])\n",
    "                return pca\n",
    "            except LinAlgError as e:\n",
    "                pass\n",
    "        self.logger.warning(\"Failed to compute a Principle Component Analysis\")\n",
    "        return None\n",
    "computed_features['PCA'] = PCA(X_train,y_train,cat)\n",
    "\n",
    "# Maybe define some more...\n",
    "\n",
    "def PCAFractionOfComponentsFor95PercentVariance(X, y, categorical):\n",
    "        pca_ = computed_features[\"PCA\"]\n",
    "        if pca_ is None:\n",
    "            return np.NaN\n",
    "        sum_ = 0.\n",
    "        idx = 0\n",
    "        while sum_ < 0.95 and idx < len(pca_.explained_variance_ratio_):\n",
    "            sum_ += pca_.explained_variance_ratio_[idx]\n",
    "            idx += 1\n",
    "        return float(idx)/float(X.shape[1])\n",
    "computed_features['PCAFractionOfComponentsFor95PercentVariance'] = PCAFractionOfComponentsFor95PercentVariance(X_train,y_train,cat)\n",
    "\n",
    "# Kurtosis of first PC\n",
    "def PCAKurtosisFirstPC(X, y, categorical):\n",
    "        pca_ = computed_features[\"PCA\"]\n",
    "        if pca_ is None:\n",
    "            return np.NaN\n",
    "        components = pca_.components_\n",
    "        pca_.components_ = components[:1]\n",
    "        transformed = pca_.transform(X)\n",
    "        pca_.components_ = components\n",
    "\n",
    "        kurtosis = scipy.stats.kurtosis(transformed)\n",
    "        return kurtosis[0]\n",
    "computed_features['PCAKurtosisFirstPC'] = PCAKurtosisFirstPC(X_train,y_train,cat)\n",
    "\n",
    "# Skewness of first PC\n",
    "def PCASkewnessFirstPC(X, y, categorical):\n",
    "        pca_ = computed_features[\"PCA\"]\n",
    "        if pca_ is None:\n",
    "            return np.NaN\n",
    "        components = pca_.components_\n",
    "        pca_.components_ = components[:1]\n",
    "        transformed = pca_.transform(X)\n",
    "        pca_.components_ = components\n",
    "\n",
    "        skewness = scipy.stats.skew(transformed)\n",
    "        return skewness[0]\n",
    "computed_features['PCASkewnessFirstPC'] = PCASkewnessFirstPC(X_train,y_train,cat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_all_metafeatures(X, y, categorical, dataset_name,\n",
    "        calculate=None, dont_calculate=None, densify_threshold=1000):\n",
    "    logger = get_logger(__name__)\n",
    "\n",
    "    \"\"\"Calculate all metafeatures.\"\"\"\n",
    "    #helper_functions.clear()\n",
    "    computed_features.clear()\n",
    "    metafeatures.clear()\n",
    "    mf_ = dict()\n",
    "\n",
    "    visited = set()\n",
    "    to_visit = deque()\n",
    "    to_visit.extend(metafeatures)\n",
    "\n",
    "    X_transformed = None\n",
    "    y_transformed = None\n",
    "\n",
    "    # TODO calculate the numpy metafeatures after all others to consume less\n",
    "    # memory\n",
    "    while len(to_visit) > 0:\n",
    "        name = to_visit.pop()\n",
    "        if calculate is not None and name not in calculate:\n",
    "            continue\n",
    "        if dont_calculate is not None and name in dont_calculate:\n",
    "            continue\n",
    "\n",
    "        if name in npy_metafeatures:\n",
    "            if X_transformed is None:\n",
    "                # TODO make sure this is done as efficient as possible (no copy for\n",
    "                # sparse matrices because of wrong sparse format)\n",
    "                sparse = scipy.sparse.issparse(X)\n",
    "                if any(categorical):\n",
    "                    ohe = OneHotEncoder(categorical_features=categorical, sparse=True)\n",
    "                    X_transformed = ohe.fit_transform(X)\n",
    "                else:\n",
    "                    X_transformed = X\n",
    "                imputer = Imputer(strategy='mean', copy=False)\n",
    "                X_transformed = imputer.fit_transform(X_transformed)\n",
    "                center = not scipy.sparse.isspmatrix(X_transformed)\n",
    "                standard_scaler = StandardScaler(copy=False, with_mean=center)\n",
    "                X_transformed = standard_scaler.fit_transform(X_transformed)\n",
    "                categorical_transformed = [False] * X_transformed.shape[1]\n",
    "\n",
    "                # Densify the transformed matrix\n",
    "                if not sparse and scipy.sparse.issparse(X_transformed):\n",
    "                    bytes_per_float = X_transformed.dtype.itemsize\n",
    "                    num_elements = X_transformed.shape[0] * X_transformed.shape[1]\n",
    "                    megabytes_required = num_elements * bytes_per_float / 1000 / 1000\n",
    "                    if megabytes_required < densify_threshold:\n",
    "                        X_transformed = X_transformed.todense()\n",
    "\n",
    "                # This is not only important for datasets which are somehow\n",
    "                # sorted in a strange way, but also prevents lda from failing in\n",
    "                # some cases.\n",
    "                # Because this is advanced indexing, a copy of the data is returned!!!\n",
    "                X_transformed = check_array(X_transformed,\n",
    "                                            force_all_finite=True,\n",
    "                                            accept_sparse='csr')\n",
    "                rs = np.random.RandomState(42)\n",
    "                indices = np.arange(X_transformed.shape[0])\n",
    "                rs.shuffle(indices)\n",
    "                # TODO Shuffle inplace\n",
    "                X_transformed = X_transformed[indices]\n",
    "                y_transformed = y[indices]\n",
    "\n",
    "            X_ = X_transformed\n",
    "            y_ = y_transformed\n",
    "            categorical_ = categorical_transformed\n",
    "        else:\n",
    "            X_ = X\n",
    "            y_ = y\n",
    "            categorical_ = categorical\n",
    "\n",
    "        dependency = metafeatures.get_dependency(name)\n",
    "        if dependency is not None:\n",
    "            is_metafeature = dependency in metafeatures\n",
    "            #is_helper_function = dependency in helper_functions\n",
    "            is_computed_features = dependency in computed_features\n",
    "\n",
    "            if is_metafeature and is_computed_features:\n",
    "                raise NotImplementedError()\n",
    "            elif not is_metafeature and not is_computed_features:\n",
    "                raise ValueError(dependency)\n",
    "            elif is_metafeature and not metafeatures.is_calculated(dependency):\n",
    "                to_visit.appendleft(name)\n",
    "                continue\n",
    "            elif is_computed_features and not computed_features.is_calculated(\n",
    "                    dependency):\n",
    "                logger.info(\"%s: Going to calculate: %s\", dataset_name,\n",
    "                            dependency)\n",
    "                value = computed_features[dependency](X_, y_, categorical_)\n",
    "                computed_features.set_value(dependency, value)\n",
    "                mf_[dependency] = value\n",
    "\n",
    "        logger.info(\"%s: Going to calculate: %s\", dataset_name,\n",
    "                    name)\n",
    "\n",
    "        value = metafeatures[name](X_, y_, categorical_)\n",
    "        metafeatures.set_value(name, value)\n",
    "        mf_[name] = value\n",
    "        visited.add(name)\n",
    "\n",
    "    mf_ = DatasetMetafeatures(dataset_name, mf_)\n",
    "    return mf_\n",
    "\n",
    "\n",
    "npy_metafeatures = set([\"LandmarkLDA\",\n",
    "                        \"LandmarkNaiveBayes\",\n",
    "                        \"LandmarkDecisionTree\",\n",
    "                        \"LandmarkDecisionNodeLearner\",\n",
    "                        \"LandmarkRandomNodeLearner\",\n",
    "                        \"LandmarkWorstNodeLearner\",\n",
    "                        \"Landmark1NN\",\n",
    "                        \"PCAFractionOfComponentsFor95PercentVariance\",\n",
    "                        \"PCAKurtosisFirstPC\",\n",
    "                        \"PCASkewnessFirstPC\",\n",
    "                        \"Skewnesses\",\n",
    "                        \"SkewnessMin\",\n",
    "                        \"SkewnessMax\",\n",
    "                        \"SkewnessMean\",\n",
    "                        \"SkewnessSTD\",\n",
    "                        \"Kurtosisses\",\n",
    "                        \"KurtosisMin\",\n",
    "                        \"KurtosisMax\",\n",
    "                        \"KurtosisMean\",\n",
    "                        \"KurtosisSTD\"])\n",
    "\n",
    "subsets = dict()\n",
    "# All implemented metafeatures\n",
    "subsets[\"all\"] = set(computed_features.keys())\n",
    "\n",
    "# Metafeatures used by Pfahringer et al. (2000) in the first experiment\n",
    "subsets[\"pfahringer_2000_experiment1\"] = set([\"number_of_features\",\n",
    "                                             \"number_of_numeric_features\",\n",
    "                                             \"number_of_categorical_features\",\n",
    "                                             \"number_of_classes\",\n",
    "                                             \"class_probability_max\",\n",
    "                                             \"landmark_lda\",\n",
    "                                             \"landmark_naive_bayes\",\n",
    "                                             \"landmark_decision_tree\"])\n",
    "\n",
    "# Metafeatures used by Pfahringer et al. (2000) in the second experiment\n",
    "# worst node learner not implemented yet\n",
    "\"\"\"\n",
    "pfahringer_2000_experiment2 = set([\"landmark_decision_node_learner\",\n",
    "                                   \"landmark_random_node_learner\",\n",
    "                                   \"landmark_worst_node_learner\",\n",
    "                                   \"landmark_1NN\"])\n",
    "\"\"\"\n",
    "\n",
    "# Metafeatures used by Yogatama and Mann (2014)\n",
    "subsets[\"yogotama_2014\"] = set([\"log_number_of_features\",\n",
    "                               \"log_number_of_instances\",\n",
    "                               \"number_of_classes\"])\n",
    "\n",
    "# Metafeatures used by Bardenet et al. (2013) for the AdaBoost.MH experiment\n",
    "subsets[\"bardenet_2013_boost\"] = set([\"number_of_classes\",\n",
    "                                     \"log_number_of_features\",\n",
    "                                     \"log_inverse_dataset_ratio\",\n",
    "                                     \"pca_95percent\"])\n",
    "\n",
    "# Metafeatures used by Bardenet et al. (2013) for the Neural Net experiment\n",
    "subsets[\"bardenet_2013_nn\"] = set([\"number_of_classes\",\n",
    "                                  \"log_number_of_features\",\n",
    "                                  \"log_inverse_dataset_ratio\",\n",
    "                                  \"pca_kurtosis_first_pc\",\n",
    "                                  \"pca_skewness_first_pc\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'NumberOfInstances': 15266.0,\n",
       " 'LogNumberOfInstances': 9.633383412358416,\n",
       " 'NumberofClasses': 2.0,\n",
       " 'NumberOfFeatures': 8.0,\n",
       " 'LogNumberOfFeatures': 2.0794415416798357,\n",
       " 'MissingValues': array([[False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False],\n",
       "        [False, False, False, ..., False, False, False]]),\n",
       " 'NumberOfInstancesWithMissingValues': 0.0,\n",
       " 'PercentageOfInstancesWithMissingValues': 0.0,\n",
       " 'NumberOfFeaturesWithMissingValues': 0.0,\n",
       " 'PercentageOfFeaturesWithMissingValues': 0.0,\n",
       " 'NumberOfMissingValues': 0.0,\n",
       " 'PercentageOfMissingValues': 0.0,\n",
       " 'NumberOfNumericFeatures': 8,\n",
       " 'NumberOfCategoricalFeatures': 0,\n",
       " 'RatioNumericalToNominal': 0.0,\n",
       " 'DatasetRatio': 0.0005240403511070353,\n",
       " 'LogDatasetRatio': -7.55394187067858,\n",
       " 'InverseDatasetRatio': 1908.25,\n",
       " 'LogInverseDatasetRatio': 7.55394187067858,\n",
       " 'ClassOccurences': defaultdict(float, {1: 4561.0, 0: 10705.0}),\n",
       " 'ClassProbabilityMin': 0.29876850517489845,\n",
       " 'ClassProbabilityMax': 0.7012314948251015,\n",
       " 'ClassProbabilityMean': 0.5,\n",
       " 'ClassProbabilitySTD': 0.20123149482510153,\n",
       " 'NumSymbols': [4610, 7242, 12849, 10980, 7415, 4517, 12914, 1871],\n",
       " 'SymbolsMin': 1871,\n",
       " 'SymbolsMax': 12914,\n",
       " 'SymbolsMean': 7799.75,\n",
       " 'SymbolsSTD': 3845.851913100659,\n",
       " 'SymbolsSum': 62398,\n",
       " 'ClassEntropy': 0.879780315772548,\n",
       " 'mutual_information': 0,\n",
       " 'Kurtosisses': [],\n",
       " 'KurtosisMin': 0,\n",
       " 'KurtosisMax': 0,\n",
       " 'KurtosisMean': 0,\n",
       " 'KurtosisSTD': 0,\n",
       " 'Skewnesses': [],\n",
       " 'SkewnessMin': 0,\n",
       " 'SkewnessMax': 0,\n",
       " 'SkewnessMean': 0,\n",
       " 'SkewnessSTD': 0,\n",
       " 'LandmarkLDA': 0.8367606616411309,\n",
       " 'LandmarkNaiveBayes': 0.7297920711801671,\n",
       " 'LandmarkDecisionTree': 0.8409548465386847,\n",
       " 'LandmarkDecisionNodeLearner': 0.7893370423302537,\n",
       " 'LandmarkRandomNodeLearner': 0.7121684919336768,\n",
       " 'Landmark1NN': 0.7243557183957774,\n",
       " 'PCA': PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "   svd_solver='auto', tol=0.0, whiten=False),\n",
       " 'PCAFractionOfComponentsFor95PercentVariance': 0.125,\n",
       " 'PCAKurtosisFirstPC': 1889.6511,\n",
       " 'PCASkewnessFirstPC': 36.407936}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "computed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_tasks = [233, 236, 242, 244, 246, 75090, 248, 251, 75124, 253,\n",
    "                        #75092, 258,\n",
    "                        #260, 261, 262, 75095, 266, 75097,\n",
    "                        #75099, 75159,\n",
    "                        #75100, 275, 288, 75103, 75108,\n",
    "                        #75109, 75110,\n",
    "                        #75112, 75113, 75114, 75115, 75116, 75117, 75119, 75120,\n",
    "                        #75121, 75123,\n",
    "                        #252, 75125, 75126, 75127, 75129,2119,\n",
    "                        #2120, 2122,\n",
    "                        #75132, 75133, 75139, 75141, 75142, 75143, 75146,\n",
    "                        #75148, 75150,\n",
    "                        #75153, 75154, 75230, 75156, 75157, 273,\n",
    "                        #75163, 75166,\n",
    "                        #75169, 75171,75173, 75174, 75175, 75176,\n",
    "                        #75179,\n",
    "                        #75184, 75185,75192, 75193, 75195,\n",
    "                        #75234, 75161,\n",
    "                        #75210,\n",
    "                        #75213, 75215, 75217, 75219, 75221,75223, 75134,\n",
    "                        #75225, 75226,\n",
    "                        #75227, 75231, 75232, 75233, 75101, 75235, 75236, 75237,\n",
    "                        #75178, 75239,\n",
    "75181, 75187, 75250,75243, 75182,75096]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classification_tasks =[75106, 75107, 75128, 2117,2123,2350,75197, 75172, 75177,75240,75189,\n",
    "#75196,75198,75168, 75201,75202, 75203, 75188, 75205, 75207,75212,\n",
    "#75222,75249,75248,75244, 254,3043, 75093,5108 ,75191,75098]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_id = classification_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humaira/anaconda3/lib/python3.6/site-packages/sklearn/discriminant_analysis.py:388: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    }
   ],
   "source": [
    "for i in t_id:\n",
    "    computed_features1={}\n",
    "    computed_features1['task_id'] = (i)\n",
    "    print(i)\n",
    "    X_train, y_train, X_test, y_test, cat = load_task(i)\n",
    "    computed_features1['NumberOfInstances']=NumberOfInstances(X_train,y_train,cat)\n",
    "    computed_features1['LogNumberOfInstances']=LogNumberOfInstances(X_train,y_train,cat)\n",
    "    computed_features1['NumberofClasses']=NumberofClasses(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfFeatures'] = NumberOfFeatures(X_train,y_train,cat)\n",
    "    computed_features1['LogNumberOfFeatures'] = LogNumberOfFeatures(X_train,y_train,cat)\n",
    "    computed_features['MissingValues'] = MissingValues(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfInstancesWithMissingValues'] = NumberOfInstancesWithMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['PercentageOfInstancesWithMissingValues'] = PercentageOfInstancesWithMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfFeaturesWithMissingValues'] = NumberOfFeaturesWithMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['PercentageOfFeaturesWithMissingValues'] = PercentageOfFeaturesWithMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfMissingValues'] = NumberOfMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['PercentageOfMissingValues'] = PercentageOfMissingValues(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfNumericFeatures'] = NumberOfNumericFeatures(X_train,y_train,cat)\n",
    "    computed_features1['NumberOfCategoricalFeatures'] = NumberOfCategoricalFeatures(X_train,y_train,cat)\n",
    "    computed_features1['RatioNumericalToNominal'] = RatioNumericalToNominal(X_train,y_train,cat)\n",
    "    computed_features1['DatasetRatio'] = DatasetRatio(X_train,y_train,cat)\n",
    "    computed_features1['LogDatasetRatio'] = LogDatasetRatio(X_train,y_train,cat)\n",
    "    computed_features1['InverseDatasetRatio'] = InverseDatasetRatio(X_train,y_train,cat)\n",
    "    computed_features1['LogInverseDatasetRatio'] = LogInverseDatasetRatio(X_train,y_train,cat)\n",
    "    computed_features['ClassOccurences'] = ClassOccurences(X_train,y_train,cat)\n",
    "    computed_features1['ClassProbabilityMin'] = ClassProbabilityMin(X_train,y_train,cat)\n",
    "    computed_features1['ClassProbabilityMax'] = ClassProbabilityMax(X_train,y_train,cat)\n",
    "    computed_features1['ClassProbabilityMean'] = ClassProbabilityMean(X_train,y_train,cat)\n",
    "    computed_features1['ClassProbabilitySTD'] = ClassProbabilitySTD(X_train,y_train,cat)\n",
    "    computed_features1['Kurtosisses'] = Kurtosisses(X_train,y_train,cat)\n",
    "    computed_features1['KurtosisMin'] = KurtosisMin(X_train,y_train,cat)\n",
    "    computed_features1['KurtosisMax'] = KurtosisMax(X_train,y_train,cat)\n",
    "    computed_features1['KurtosisMean'] = KurtosisMean(X_train,y_train,cat) \n",
    "    computed_features1['KurtosisSTD'] = KurtosisSTD(X_train,y_train,cat) \n",
    "    computed_features1['Skewnesses'] = Skewnesses(X_train,y_train,cat) \n",
    "    computed_features1['SkewnessMin'] = SkewnessMin(X_train,y_train,cat) \n",
    "    computed_features1['SkewnessMax'] = SkewnessMax(X_train,y_train,cat)    \n",
    "    computed_features1['SkewnessMean'] = SkewnessMean(X_train,y_train,cat) \n",
    "    computed_features1['SkewnessSTD'] = SkewnessSTD(X_train,y_train,cat)  \n",
    "    computed_features1['LandmarkLDA'] = LandmarkLDA(X_train,y_train,cat)  \n",
    "    computed_features1['LandmarkNaiveBayes'] = LandmarkNaiveBayes(X_train,y_train,cat) \n",
    "    computed_features1['LandmarkDecisionTree'] = LandmarkDecisionTree(X_train,y_train,cat)\n",
    "    computed_features1['LandmarkDecisionNodeLearner'] = LandmarkDecisionNodeLearner(X_train,y_train,cat)\n",
    "    computed_features1['LandmarkRandomNodeLearner'] = LandmarkRandomNodeLearner(X_train,y_train,cat)\n",
    "    computed_features1['Landmark1NN'] = Landmark1NN(X_train,y_train,cat)\n",
    "    computed_features['PCA'] = PCA(X_train,y_train,cat)\n",
    "    computed_features1['PCAFractionOfComponentsFor95PercentVariance'] = PCAFractionOfComponentsFor95PercentVariance(X_train,y_train,cat)\n",
    "    computed_features1['PCAKurtosisFirstPC'] = PCAKurtosisFirstPC(X_train,y_train,cat)\n",
    "    computed_features1['PCASkewnessFirstPC'] = PCASkewnessFirstPC(X_train,y_train,cat)  \n",
    "    computed_features1['mutual_information'] = mutual_information(X_train,y_train,cat)\n",
    "    computed_features1['ClassEntropy'] = ClassEntropy(X_train,y_train,cat)  \n",
    "    computed_features1['SymbolsMin'] = SymbolsMin(X_train,y_train,cat)  \n",
    "    computed_features1['SymbolsMax'] = SymbolsMax(X_train,y_train,cat) \n",
    "    computed_features1['SymbolsMean'] = SymbolsMean(X_train,y_train,cat)\n",
    "    computed_features1['SymbolsSTD'] = SymbolsSTD(X_train,y_train,cat)   \n",
    "    computed_features1['SymbolsSum'] = SymbolsSum(X_train,y_train,cat)  \n",
    "   \n",
    "    metaData=[computed_features1]\n",
    "    df = pd.DataFrame(metaData)\n",
    "    #print(df) \n",
    "    # count = count + 1;\n",
    "    # print(count)\n",
    "    # df.append(df1)\n",
    "    #df.to_csv(\"/home/humaira/my_data/meta_data_temp6.csv\")\n",
    "    df.to_csv(\"/home/humaira/my_data/meta_data_temp6.csv\",mode='a', header = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
